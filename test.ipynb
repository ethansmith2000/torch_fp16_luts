{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp_lut.py\n",
    "import torch\n",
    "import exp_lut_cuda  # The compiled extension\n",
    "\n",
    "def create_exp_lut():\n",
    "    # Create all possible FP16 bit patterns\n",
    "    bit_patterns = torch.arange(0, 65536, dtype=torch.uint16)\n",
    "    fp16_values = bit_patterns.view(torch.float16)\n",
    "\n",
    "    # Compute exp in higher precision to reduce errors\n",
    "    fp32_values = fp16_values.to(torch.float32)\n",
    "    exp_values = torch.exp(fp32_values)\n",
    "\n",
    "    # Cast back to FP16 for the LUT\n",
    "    exp_values_fp16 = exp_values.to(torch.float16)\n",
    "\n",
    "    # Transfer LUT to GPU\n",
    "    exp_lut = exp_values_fp16.cuda()\n",
    "\n",
    "    return exp_lut\n",
    "\n",
    "def exp_lut(input_tensor, lut):\n",
    "    \"\"\"\n",
    "    Compute exp using LUT.\n",
    "\n",
    "    Args:\n",
    "        input_tensor (torch.Tensor): Input tensor of dtype float16 on CUDA.\n",
    "        lut (torch.Tensor): Precomputed LUT of dtype float16 on CUDA.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Output tensor with exp applied, dtype float16 on CUDA.\n",
    "    \"\"\"\n",
    "    assert input_tensor.dtype == torch.float16, \"Input must be float16\"\n",
    "    assert input_tensor.is_cuda, \"Input must be on CUDA\"\n",
    "\n",
    "    output = torch.empty_like(input_tensor)\n",
    "    exp_lut_cuda.exp_lut_cuda(input_tensor, output, lut)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example_usage.py\n",
    "import torch\n",
    "from exp_lut import create_exp_lut, exp_lut\n",
    "\n",
    "def main():\n",
    "    # Initialize LUT\n",
    "    print(\"Creating exp LUT...\")\n",
    "    exp_lookup_table = create_exp_lut()\n",
    "\n",
    "    # Create a sample input tensor\n",
    "    print(\"Creating input tensor...\")\n",
    "    input_size = 1024 * 1024  # 1 million elements\n",
    "    input_fp16 = torch.randn(input_size, device='cuda', dtype=torch.float16)\n",
    "\n",
    "    # Compute exp using LUT\n",
    "    print(\"Computing exp using LUT...\")\n",
    "    output_fp16 = exp_lut(input_fp16, exp_lookup_table)\n",
    "\n",
    "    # Compute exp using PyTorch for verification\n",
    "    print(\"Computing exp using PyTorch...\")\n",
    "    output_ref = torch.exp(input_fp16.to(torch.float32)).to(torch.float16)\n",
    "\n",
    "    # Compare results\n",
    "    print(\"Comparing results...\")\n",
    "    max_diff = (output_fp16.to(torch.float32) - output_ref.to(torch.float32)).abs().max()\n",
    "    print(f\"Maximum difference between LUT and PyTorch: {max_diff}\")\n",
    "\n",
    "    # Check if within acceptable tolerance\n",
    "    if torch.allclose(output_fp16, output_ref, atol=1e-2):\n",
    "        print(\"LUT-based exp matches PyTorch's exp within tolerance.\")\n",
    "    else:\n",
    "        print(\"LUT-based exp does NOT match PyTorch's exp within tolerance.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold for exp(x) = +inf: x >= 13.875\n",
      "Threshold for exp(x) = 0: x <= -11.5625\n",
      "Threshold for exp(x) = 1: x <= 0.0038909912109375\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'torch.dtype' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[136], line 112\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m reduced_lut_gpu, mapping_table_gpu\n\u001b[1;32m    110\u001b[0m _, regular_exp, regular_input_bits \u001b[38;5;241m=\u001b[39m get_allowed_exp_range(dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16)\n\u001b[0;32m--> 112\u001b[0m reduced_lut, mapping_table \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_reduced_lut_and_mapping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mregular_input_bits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregular_exp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReduced LUT size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreduced_lut\u001b[38;5;241m.\u001b[39mnumel()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m entries\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMapping table size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmapping_table\u001b[38;5;241m.\u001b[39mnumel()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m entries\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[136], line 83\u001b[0m, in \u001b[0;36mcreate_reduced_lut_and_mapping\u001b[0;34m(regular_input_bits, regular_exp)\u001b[0m\n\u001b[1;32m     80\u001b[0m SPECIAL_ZERO \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Handle +inf\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m inf_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(torch\u001b[38;5;241m.\u001b[39misinf(torch\u001b[38;5;241m.\u001b[39mexp(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m32768\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32768\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint16\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m)))[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     86\u001b[0m mapping_table[inf_indices] \u001b[38;5;241m=\u001b[39m SPECIAL_INF\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Handle exp(x) = 1 (x = 0)\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'torch.dtype' object is not callable"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def next_fp16(x, dtype=torch.float16, increment=1):\n",
    "    \"\"\"\n",
    "    gets immediate next number representable in FP16\n",
    "    \"\"\"\n",
    "    if not isinstance(x, torch.Tensor):\n",
    "        x = torch.tensor(x, dtype=dtype)\n",
    "    else:\n",
    "        x = x.to(dtype)\n",
    "    \n",
    "    # View the bits as int16\n",
    "    x_bits = x.view(torch.int16)\n",
    "    next_bits = x_bits + increment\n",
    "    next_x = next_bits.view(dtype)\n",
    "    \n",
    "    return next_x\n",
    "\n",
    "\n",
    "def get_allowed_exp_range(dtype=torch.bfloat16, low_threshold=1e-5, high_threshold=65536 * 16):\n",
    "    # Create all FP16 values\n",
    "    bit_patterns = torch.arange(-65536//2,65536//2, dtype=torch.int16)\n",
    "    fp16_values = bit_patterns.view(dtype)\n",
    "\n",
    "    # exp\n",
    "    exp_fp16 = torch.exp(fp16_values) \n",
    "\n",
    "    # where it goes to inf\n",
    "    if high_threshold is not None:\n",
    "        is_inf = exp_fp16 > high_threshold\n",
    "    is_inf = is_inf | torch.isinf(exp_fp16) #| torch.isnan(exp_fp16)\n",
    "\n",
    "    # where it goes to zero\n",
    "    zero_thresh = low_threshold or torch.finfo(dtype).tiny\n",
    "    is_zero = exp_fp16 < zero_thresh\n",
    "\n",
    "    # where it goes to one,\n",
    "    # find number representable just above 1\n",
    "    next_fp16_one = next_fp16(1, dtype=dtype, increment=1)\n",
    "    previous_fp16_one = next_fp16(1, dtype=dtype, increment=-1)\n",
    "    is_one = (exp_fp16 < next_fp16_one) & (exp_fp16 > previous_fp16_one)\n",
    "\n",
    "    # Determine the thresholds\n",
    "    x_inf = fp16_values[is_inf].min()\n",
    "    x_zero = fp16_values[is_zero].max()\n",
    "    x_one = fp16_values[is_one].max()\n",
    "\n",
    "    is_nan = torch.isnan(fp16_values)\n",
    "\n",
    "    # remaining available values\n",
    "    mask = ~(is_inf | is_zero | is_one | is_nan)\n",
    "    allowed_fp16_values = fp16_values[mask]\n",
    "    allowed_exp_fp16 = exp_fp16[mask]\n",
    "    allowed_bits = bit_patterns[mask]\n",
    "\n",
    "    print(f\"Threshold for exp(x) = +inf: x >= {x_inf.item()}\")\n",
    "    print(f\"Threshold for exp(x) = 0: x <= {x_zero.item()}\")\n",
    "    print(f\"Threshold for exp(x) = 1: x <= {x_one.item()}\")\n",
    "\n",
    "    lut_indices = torch.arange(0, allowed_fp16_values.numel(), dtype=torch.int16)\n",
    "    mapping_table = torch.zeros(65536, dtype=torch.int16)\n",
    "    mapping_table[allowed_bits] = lut_indices\n",
    "\n",
    "    # Define special case codes:\n",
    "    # -1: +inf, -2: 1, -3: 0, -4 nan\n",
    "    SPECIAL_INF = -1\n",
    "    SPECIAL_ONE = -2\n",
    "    SPECIAL_ZERO = -3\n",
    "    SPECIAL_NAN = -4\n",
    "\n",
    "    mapping_table[is_inf] = -1\n",
    "    mapping_table[is_one] = -2\n",
    "    mapping_table[is_zero] = -3\n",
    "    mapping_table[is_nan] = -4\n",
    "\n",
    "    return allowed_fp16_values, allowed_exp_fp16, allowed_bits, is_inf, is_zero, is_one, is_nan, mapping_table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
