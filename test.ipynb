{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp_lut.py\n",
    "import torch\n",
    "import exp_lut_cuda  # The compiled extension\n",
    "\n",
    "def create_exp_lut():\n",
    "    # Create all possible FP16 bit patterns\n",
    "    bit_patterns = torch.arange(0, 65536, dtype=torch.uint16)\n",
    "    fp16_values = bit_patterns.view(torch.float16)\n",
    "\n",
    "    # Compute exp in higher precision to reduce errors\n",
    "    fp32_values = fp16_values.to(torch.float32)\n",
    "    exp_values = torch.exp(fp32_values)\n",
    "\n",
    "    # Cast back to FP16 for the LUT\n",
    "    exp_values_fp16 = exp_values.to(torch.float16)\n",
    "\n",
    "    # Transfer LUT to GPU\n",
    "    exp_lut = exp_values_fp16.cuda()\n",
    "\n",
    "    return exp_lut\n",
    "\n",
    "def exp_lut(input_tensor, lut):\n",
    "    \"\"\"\n",
    "    Compute exp using LUT.\n",
    "\n",
    "    Args:\n",
    "        input_tensor (torch.Tensor): Input tensor of dtype float16 on CUDA.\n",
    "        lut (torch.Tensor): Precomputed LUT of dtype float16 on CUDA.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Output tensor with exp applied, dtype float16 on CUDA.\n",
    "    \"\"\"\n",
    "    assert input_tensor.dtype == torch.float16, \"Input must be float16\"\n",
    "    assert input_tensor.is_cuda, \"Input must be on CUDA\"\n",
    "\n",
    "    output = torch.empty_like(input_tensor)\n",
    "    exp_lut_cuda.exp_lut_cuda(input_tensor, output, lut)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example_usage.py\n",
    "import torch\n",
    "from exp_lut import create_exp_lut, exp_lut\n",
    "\n",
    "def main():\n",
    "    # Initialize LUT\n",
    "    print(\"Creating exp LUT...\")\n",
    "    exp_lookup_table = create_exp_lut()\n",
    "\n",
    "    # Create a sample input tensor\n",
    "    print(\"Creating input tensor...\")\n",
    "    input_size = 1024 * 1024  # 1 million elements\n",
    "    input_fp16 = torch.randn(input_size, device='cuda', dtype=torch.float16)\n",
    "\n",
    "    # Compute exp using LUT\n",
    "    print(\"Computing exp using LUT...\")\n",
    "    output_fp16 = exp_lut(input_fp16, exp_lookup_table)\n",
    "\n",
    "    # Compute exp using PyTorch for verification\n",
    "    print(\"Computing exp using PyTorch...\")\n",
    "    output_ref = torch.exp(input_fp16.to(torch.float32)).to(torch.float16)\n",
    "\n",
    "    # Compare results\n",
    "    print(\"Comparing results...\")\n",
    "    max_diff = (output_fp16.to(torch.float32) - output_ref.to(torch.float32)).abs().max()\n",
    "    print(f\"Maximum difference between LUT and PyTorch: {max_diff}\")\n",
    "\n",
    "    # Check if within acceptable tolerance\n",
    "    if torch.allclose(output_fp16, output_ref, atol=1e-2):\n",
    "        print(\"LUT-based exp matches PyTorch's exp within tolerance.\")\n",
    "    else:\n",
    "        print(\"LUT-based exp does NOT match PyTorch's exp within tolerance.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
